\documentclass[dvipdfmx]{beamer}
\usepackage{tutorial}
\title{計算機実験 --- 線形回帰・カーネル法}

\begin{document}

\lstset{language={C},basicstyle=\ttfamily\scriptsize,showspaces=false,rulecolor=\color[cmyk]{0, 0.29,0.84,0}}

\begin{frame}
  \titlepage
  \tableofcontents
\end{frame}

\section{最小二乗法による回帰分析}

\begin{frame}[t,fragile]{最小二乗法によるフィッティング}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item 説明変数(例: 電圧): $x_1,x_2,x_3,\cdots,$
  \item 観測値(例: 電流): $y_1,y_2,y_3,\cdots,$
  \item 単回帰モデル: $y=a+bx+\epsilon$ \ \ ($\epsilon$: ノイズ)
  \item 未知母数: $a,b$
  \item 最小二乗法:
    残差$\displaystyle R(a,b) = \sum_i (y_i - (a+bx_i))^2$を最小化
    \[
    \begin{split}
      \frac{\partial R}{\partial a} &= - 2 \sum_i (y_i - (a+bx_i)) = 0 \\
      \frac{\partial R}{\partial b} &= - 2 \sum_i (y_i - (a+bx_i))x_i = 0
    \end{split}
    \]
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{回帰分析の一般化}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item 基底関数: $\phi_j(x)$ \ \ ($j=1 \cdots M$)
  \item モデル: $\displaystyle y(x) = \sum_j \phi_j(x) w_j + \epsilon$
  \item 残差: $\displaystyle R({\bf w}) = \sum_i (y_i - \sum_j \phi_j(x_i) w_j)^2$
    \[
    \frac{\partial R}{\partial w_k} = -2 \sum_i (y_i - \sum_j \phi_j(x_i) w_j) \phi_k(x_i) = 0
    \]
  \item 計画行列(design matrix) $\Phi_{ij} = \phi_j(x_i)$を導入すると
    \[
    R({\bf w}) = | {\bf y} - \Phi {\bf w} |^2
    \]
  \item 最小二乗解: $\Phi^{\rm t} \Phi {\bf w} = \Phi^{\rm t} {\bf y} \ \ \Rightarrow \ \
{\bf w} = (\Phi^{\rm t} \Phi)^{-1}\Phi^{\rm t} {\bf y}$
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{リッジ回帰(Ridge Regression)}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item 基底関数の数(例: 多項式の次数)を増やしすぎると過学習 (over-fitting)」が生じる
  \item 正則化最小二乗法 ($\lambda$は非負の定数)
    \begin{align*}
    R({\bf w}) &= \sum_i (y_i - \sum_j \phi_j(x_i) w_j)^2 + {\color{red} \lambda} \sum_j w_j^2 \\
    &= | {\bf y} - \Phi {\bf w} |^2 + \lambda {\bf w}^{\rm t} {\bf w}
    \end{align*}
  \item 最小二乗解
    \[
    (\Phi^{\rm t} \Phi + \lambda \, {\rm I}) {\bf w} = \Phi^{\rm t} {\bf y} \ \ \Rightarrow \ \ 
      {\bf w} = (\Phi^{\rm t} \Phi + \lambda \, {\rm I})^{-1}\Phi^{\rm t} {\bf y}
      \]
  \end{itemize}
\end{frame}

\section{カーネル法}

\begin{frame}[t,fragile]{カーネルトリック}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item 方程式を変形 ${\bf w} = \frac{1}{\lambda} \Phi^{\rm t} ({\bf y} -\Phi {\bf w})$
  \item $\alpha = \frac{1}{\lambda}({\bf y} -\Phi {\bf w})$と定義すると${\bf w} =\Phi^{\rm t} \alpha$
    \item $w$は$\begin{pmatrix} \phi_1(x_1) \\ \vdots \\ \phi_M(x_1) \end{pmatrix} \cdots 
      \begin{pmatrix} \phi_1(x_N) \\ \vdots \\ \phi_M(x_N) \end{pmatrix}$の線形結合
    \item $M$次元中の$N$次元部分空間にある ($M$: 基底関数の数、$N$: サンプル数)
    \item 基底関数を増やしても自由度は増えない
    \item $w$を求める代わりに、直接$\alpha$を求めても良い (リプリゼンター定理)
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{カーネルによる線形回帰}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item 残差$R$を$\alpha$をつかって表現
    \[
    R(\alpha) = | y - \Phi \Phi^{\rm t} \alpha |^2 + \lambda \alpha^{\rm t} \Phi \Phi^{\rm t} \alpha
    \]
  \item グラム行列(Gram matrix) $K \equiv \Phi \Phi^{\rm t}$を導入すると
    \[
    R(\alpha) = | y - K \alpha |^2 + \lambda \alpha^{\rm t} K \alpha
    \]
  \item グラム行列($N \times N$対称行列)の成分
    \[
    K_{ik} = \sum_j \Phi_{ij} \Phi_{kj} = \sum_j \phi_j(x_i) \phi_j(x_k) \equiv {\color{red} k(x_i,x_k)}
    \]
  \item $M$個の基底関数の組を考えるかわりに1つのカーネル関数$k(x,x')$を導入すればよい(カーネル法)
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{カーネルによる線形回帰}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item 残差$R$の最小化
    \[
    \alpha = (K + \lambda \, {\rm I})^{-1} {\bf y}
    \]
  \item 点$x$における$y$の推定値
    \begin{align*}
    y &= \sum_j \phi_j(x) w_j = \sum_{i,j} \phi_j(x) \phi_j(x_i) \alpha_i = \sum_i k(x_i,x) \alpha_i \\ &= k^{\rm t}(x) \alpha = k^{\rm t}(x) (K + \lambda \, {\rm I})^{-1} {\bf y}
k(x) = \begin{pmatrix} k(x_1,x) \\ k(x_2,x) \\ \vdots \\ k(x_N,x) \end{pmatrix}
    \end{align*}
  \item 例: ガウシアンカーネル $k(x,x') = \exp(-\beta|x-x'|^2)$
  \end{itemize}
\end{frame}

\section{ベイズ統計の基礎}

\begin{frame}[t,fragile]{条件付き確率}
  \begin{itemize}
    \setlength{\itemsep}{1em}
  \item 子供が二人いる家族において「二人の子供のうち少くとも一人が女の子である場合」二人とも女の子である確率は?
  \item 日本人の{\color{red} 1/10000}がウイルスAに感染しているとする。このウイルスに感染していると、{\color{red} 999/1000}の確率で検査で陽性となる。一方、感染していなくても、{\color{red} 1/100}の確率で陽性となってしまう(偽陽性)。検査結果が陽性の場合、感染している可能性は?
    \begin{itemize}
    \item 事象A：ウイルスに感染
    \item 事象B：検査で陽性
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{ベイズの定理}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item 確率に対する公式
    \begin{align*}
      P(A \cap B) &= P(B|A) P(A) = P(A|B) P(B) \\ P(B) &= P(B|A) P(A) + P(B|\bar{A}) P(\bar{A})
    \end{align*}
  \item ベイズの定理 (確率の反転)
    \[
    P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{P(B|A)P(A)}{P(B|A) P(A) + P(B|\bar{A}) P(\bar{A})}
    \]
  \item 検査が陽性でも、実際に感染している可能性(確率)は
    \[
    \frac{\frac{999}{1000} \cdot \frac{1}{10000}}{\frac{999}{1000} \cdot \frac{1}{10000} + \frac{1}{100} \cdot \frac{9999}{10000}} \approx {\color{red} 0.009}
    \]
    \begin{itemize}
    \item 予想よりずっと小さい?
    \item 「検査が陽性」という事実により、感染している確率が 0.01\%から 0.9\%に増加 $\Rightarrow$ ベイズ統計・機械学習の基礎公式
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{ベイズ統計学(Bayesian Statistics)}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item 条件つき確率におけるベイズの定理
    \[
    p(\theta|y) \sim p(y|\theta) p(\theta)
    \]
  \item この(あたりまえの)関係式をベイズ統計では以下のように解釈する
    \begin{itemize}
    \item $\theta$未知母数(パラメータ) : 物理量の平均、分散、比例係数、etc
    (確定した値ではなくある分布に従って変動する量として考える)
    \item $y$ 観測値 : すでに「与えられた」確定したものと考える
    \item $p(\theta)$ 事前確率 (prior probability) : 母数に関する何らかの事前情報
    \item $p(y|\theta)$ 尤度 (likelihood) : $y$は与えられている$\theta$の関数と解釈。$l(\theta|y)$ と書く
    \item $p(\theta|y)$ 事後確率 (posterior probability) : 観測で情報が増えた後の$\theta$の確率分布
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{頻度主義的アプローチとベイズ的アプローチ}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item コインを一回投げて表が出る確率 $q$
  \item 連続して三回連続して表が出る($y$)確率 $p(y|q) = q^3 = l(q|y)$  ($q$ の尤度関数)
  \item 頻度主義的アプローチ(最尤推定)
    \begin{itemize}
    \item $q$ の推定値 $q=1$ \ \ $\Rightarrow$ \ \ 未来永劫表が出続ける!
    \item 観測データ数$\sim$母数の数の時 \ \ $\Rightarrow$ \ \ 過学習(over-fitting)
    \end{itemize}
  \item ベイズ的アプローチ
    \begin{itemize}
    \item $q$の事前分布として $p(q) = 1$  \ \ $\Rightarrow$ \ \ %事後分布
      $p(q|y) \sim l(q|y) p(q) = q^3$
    \end{itemize}
  \item ベイズ的アプローチでは過学習の問題が生じない
  \item 有効パラメータ数が自動的にデータ集合のサイズに適合
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{回帰分析への応用}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item 単回帰モデル: $y=a+bx+\epsilon$ \ \ ($\epsilon$: ノイズ)
  \item 尤度関数 (likelihood function) :
    \[
    l(a, b | \{x_i\}, \{y_i\}) = p(\{y_i\} | \{x_i\}, a, b) \sim \prod_i \exp \Big[ -\frac{(y_i - (a+bx_i))^2}{2\sigma^2}\Big]
    \]
  \item 事前分布を導入
    \begin{itemize}
    \item 例えば $a$, $b$ について ${\cal N}(0,1000)$
    \end{itemize}
  \item 事後分布:
    \[
    p(a,b | \{x_i\}, \{y_i\}) \sim l(a, b | \{x_i\}, \{y_i\}) p(a) p(b)
    \]
    \begin{itemize}
      \item $a$, $b$ の事後周辺分布から期待値とその確からしさを推定
      \item 最大事後確率(MAP)推定
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{最小二乗法・カーネル法・ベイズ推定}
  \begin{itemize}
    \setlength{\itemsep}{1em}
  \item 非線形最小二乗法
    \begin{itemize}
    \item 非線形の最小化問題
    \end{itemize}
  \item カーネル法
    \begin{itemize}
    \item 実質的に無限次元の問題を解くことができる
    \item データサイズが大きくなるとその3乗に比例して計算量が増大
    \end{itemize}
  \item ベイズ推定
    \begin{itemize}
    \item 事後確率をコンパクトな形で求めることは難しい
    \item モンテカルロ法の利用
    \end{itemize}    
  \end{itemize}
\end{frame}

\section{実習その5}

\begin{frame}[t,fragile]{EX5-1: 最小二乗フィッティング}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item[5-1-1] \href{https://github.com/todo-group/computer-experiments/exercise/linear_regression/regression.c}{exercise/linear\_regression/regression.c}は数値データを読み込み、一次式で最小二乗フィッテイングを行うプログラムである。実行すると最終行に一次式の係数が出力される。
\begin{lstlisting}
$ ./regression measurement1.dat
\end{lstlisting}
    ファイル{\tt measurement1.dat}の1カラム目は$x$、2カラム目は$y$、3カラム目は$y$の誤差の値(計算の中では使っていない)である。元データとフィッティング結果をグラフにせよ
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{EX5-2: 高次関数でのフィッティング}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item[5-2-1] {\tt regression.c}中で、15行目の{\tt nbase}は基底関数の数を表し、18行目からの関数{\tt f}は{\tt i}番目の基底関数の$x$における値を返す関数である。二次式でフィッティングが行えるよう、15行目から26行目を修正し、実行結果をグラフにせよ。一方、{\tt measurement2.dat}では、なめらかなバックグラウンドの上に中心$\mu=3.3$、分散$\sigma^2=1.3$のGaussianが乗っていることが分かっている。{\tt regression.c}を修正し、Gaussianの係数の大きさを見積もってみよ
  \end{itemize}
\end{frame}

\begin{frame}[t,fragile]{EX5-3: 応用課題}
  \begin{itemize}
    %\setlength{\itemsep}{1em}
  \item[5-3-1] \href{https://github.com/todo-group/computer-experiments/exercise/linear_regression/regression_lu.c}{\tt exercise/linear\_regression/regression\_lu.c}はLU分解により最小二乗法の解を求めるプログラムである。{\tt regression.c}と同じ結果が出力されることを確認せよ。基底関数の中に互いに線形独立でないものがある場合には、{\tt regression\_lu.c}はエラーとなることを確認し、その理由について考察せよ。一方で、{\tt regression.c}は動作するが、どのような解を与えるか?
  \item[5-3-2] {\tt regression.c}、{\tt regression\_lu.c}では、測定量の誤差の値は使っていない。残差を誤差で重み付けするようにプログラムを改良せよ。また、フィッティング結果(係数)の誤差はどのようにすれば見積もることができるか?
  \item [5-3-3] 物理学実験で得られたデータをフィッティングしてみよう。フィッティング関数が係数に関して非線形である場合は、どのように解を求めればよいか?
  \end{itemize}
\end{frame}

\end{document}
